---
title: Coursera Machine Learning Project - Use Machine Learning Algorithm to Classify
  Exercise Method Based Wearable Fitness Tracker Data
author: "Steven Oshry"
date: "Sunday, May 24, 2015"
output: html_document
fontsize: 8pt
---
##### Overview 


Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

This project uses machine learning (specifically bossted regression trees) to classify the type of lift being performed based on the exercise data from the wearable device.

Read more: 

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.


##### Read in the data set

The Rcode below calls the libraries that will be needed and then reads in the data.

```{r , echo=TRUE, message=FALSE, cache=TRUE}
suppressMessages(library(caret)) 
suppressMessages(library(ggplot2))
suppressMessages(library(corrplot))
suppressMessages(library(RCurl))
setwd("~/Coursera/Machine_learning")

training <- read.csv("./data/pml-training.csv", header= TRUE,
                     sep=",", 
                      na.strings = c("NA", "#DIV/0!","")
                     )

```


##### Examine data set for missing variables, Clean data and then split into test and training

 The testing dataset on the website is only the 20 observations to test
 the classification datbase so for the purposes of testing the classification model that will be developed, the data set will be randomly split into test and training datasets with 60% in the training dataset.
First, each column will be checked for missing data and only columns with fewer than 60% of missing observations will be kept


```{r , echo=TRUE, cache=TRUE}

#calculate % missing for each column

var_smry<-  data.frame(colSums(is.na(training))/nrow(training))
names(var_smry)[1]<-"pct_na"
# keep only variables with < 60% missing obs
var_smry2 <-c(row.names(subset(var_smry, pct_na<0.6)))
df_clean <-training[,var_smry2]

# create model variables
modl_vars<-names(df_clean)[-which(names(df_clean) 
                                      %in% names(df_clean)[1:7])]
                                 
modl_df<- df_clean[,modl_vars]

# split 60% into training, remaing 40% as test

inTrain <- createDataPartition(y=df_clean$classe,
                               p=0.6, list=FALSE)

training <- df_clean[inTrain,]
testing <- df_clean[-inTrain,]

#keep only variables to model, getting rid of descriptors
training_modl<-training[,modl_vars]
testing_modl<-testing[,modl_vars]

```
##### Exploratory data Analysis , check Correlations
In the the correlation plot shown below there are many high correlations.  Principal components is a way of reducing the dimensionality in the data while all the components are uncorrelated (orthogonal) to each other.


```{r test, echo=FALSE,fig.width=9, fig.height = 5, fig.align='center'}
options( warn = -1)
#set warnings back to 0 to tun back on
# see if any varaibles have high correlations
library(corrplot)
cor_mat <-cor(training_modl[,  names(training_modl)[1:20]] )

corrplot(cor_mat,method="shade", shade.col=NA, tl.col="black", tl.srt=45)
```

##### Principal Component analysis

Principal components is a way of reducing the dimensionality in the data while all the components are uncorrelated (orthogonal) to each other. The code below
shows that 18 principal components explain 90% of the variance among the explanatory variables.


```{r , echo=TRUE, cache=TRUE}

set.seed(988889)
preProc <- preProcess(training_modl[,  !names(training_modl) %in% c("classe")]
                      ,method="pca",thresh=0.90)
preProc

```
##### Machine learning algortihm to predict class of exercise (classe)  from data

The initial choice was to use a random forest algorithm but my PC only has 4G of RAM so itcould not handle random forest.

The following code uses the gbm algorithm on the principal components that were just created above. The gbm algorith is boosted regression trees.  This procedure attempts to minimize the error of the preceeding trees.   Please note  - I would have used 10 fold cross validation but ran into memory issues on my PC
```{r , echo=TRUE, cache=TRUE}
cvCtrl <- trainControl(#5 fold cv 
                     method="repeatedcv", 
                     #repeat 1 times
                     repeats=1, 
                       number=5,
                       classProbs=TRUE )
trainPC <- predict(preProc,
                   training_modl[,!names(training_modl)
                                         %in% c("classe")])



gbmFit1 <- train(training_modl$classe ~ ., data=trainPC,
                 method = "gbm",
                 trControl = cvCtrl,
                 verbose = FALSE)
```
##### Model Summary  and estimate of error

  The standard deviation of the model accurracy is 0.006 which shows that there is very little variability in the accuracy.  This is one of the advantages of using boosted regression trees. 


```{r , echo=TRUE, cache=TRUE}
print(gbmFit1)

```


##### Summary of Performance on Training data
The accuracy of this model is 84%. I believe I could have improved the accuracy if it was able to be run with 10 fold cross validation instead of 5.  However to to RAM issues, 5 fold was the highest that could be run.


```{r , echo=FALSE, cache=TRUE}
conf_train_modl1<-
        confusionMatrix(training_modl$classe,predict(gbmFit1,trainPC))
conf_train_modl1
```
##### Summary of Performance on Test data.
The accuracy on the test (holdout) data is 80% which is very close to the training data set accuracy.  This indicates that the model was not overfit.

```{r , echo=FALSE, cache=TRUE}
testPC <- predict(preProc,
                  testing_modl[,-which(names(testing_modl)
                                       %in% c("classe"))])

conf_test_modl1<-confusionMatrix(testing_modl$classe,predict(gbmFit1,testPC))
conf_test_modl1

```

##### Conclusion 
By using principal components to reduce the dimensionality of the data and then using boosted regression trees, the model was able to accurately predict the exercise method in 80% of the cases based on the exercise monitor data.  The acccuracy possible could have been improved by increasing the number of folds in the cross validation but the computer could not run it due to memory issues.

